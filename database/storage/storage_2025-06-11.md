# arxiv-daily
 Automated deployment @ 2025-06-11 10:03:46
> Add your topics and keywords in `database/topic.yml` 
> You can also view historical data through the `database/storage` 

## Mutimodal

### Weakly Supervised grounding
|Publish Date|Title|Authors|PDF|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-03-05**|**Cross-modal Causal Relation Alignment for Video Question Grounding**|Weixing Chen et.al.|[2503.07635v1](http://arxiv.org/abs/2503.07635v1)|null|
|**2024-02-29**|**How to Understand "Support"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding**|Jiamin Luo et.al.|[2402.19116v2](http://arxiv.org/abs/2402.19116v2)|null|
|**2024-01-19**|**Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering**|Haibo Wang et.al.|[2401.10711v4](http://arxiv.org/abs/2401.10711v4)|[link](https://github.com/whb139426/gcg)|
|**2023-12-15**|**Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment**|Xiaoxu Xu et.al.|[2312.09625v4](http://arxiv.org/abs/2312.09625v4)|null|
|**2023-12-07**|**Improved Visual Grounding through Self-Consistent Explanations**|Ruozhen He et.al.|[2312.04554v1](http://arxiv.org/abs/2312.04554v1)|null|
|**2023-05-18**|**Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement**|Davide Rigoni et.al.|[2305.10913v2](http://arxiv.org/abs/2305.10913v2)|[link](https://github.com/drigoni/sprm)|
|**2023-03-31**|**Zero-shot Referring Image Segmentation with Global-Local Context Features**|Seonghoon Yu et.al.|[2303.17811v2](http://arxiv.org/abs/2303.17811v2)|[link](https://github.com/seonghoon-yu/zero-shot-ris)|
|**2022-10-09**|**MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning**|Zijia Zhao et.al.|[2210.04183v3](http://arxiv.org/abs/2210.04183v3)|null|
|**2022-06-14**|**Beyond Grounding: Extracting Fine-Grained Event Hierarchies Across Modalities**|Hammad A. Ayyubi et.al.|[2206.07207v3](http://arxiv.org/abs/2206.07207v3)|null|
|**2022-04-22**|**Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering**|Yu-Jung Heo et.al.|[2204.10448v1](http://arxiv.org/abs/2204.10448v1)|[link](https://github.com/yujungheo/kbvqa-public)|
|**2022-03-16**|**Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding**|Haojun Jiang et.al.|[2203.08481v2](http://arxiv.org/abs/2203.08481v2)|[link](https://github.com/leaplabthu/pseudo-q)|
|**2022-02-09**|**Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?**|Jiawen Zhang et.al.|[2202.04306v1](http://arxiv.org/abs/2202.04306v1)|null|
|**2021-12-01**|**Weakly-Supervised Video Object Grounding via Causal Intervention**|Wei Wang et.al.|[2112.00475v1](http://arxiv.org/abs/2112.00475v1)|null|
|**2021-09-04**|**Weakly Supervised Relative Spatial Reasoning for Visual Question Answering**|Pratyay Banerjee et.al.|[2109.01934v1](http://arxiv.org/abs/2109.01934v1)|null|
|**2020-10-12**|**MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding**|Qinxin Wang et.al.|[2010.05379v1](http://arxiv.org/abs/2010.05379v1)|[link](https://github.com/qinzzz/Multimodal-Alignment-Framework)|
|**2020-06-17**|**Contrastive Learning for Weakly Supervised Phrase Grounding**|Tanmay Gupta et.al.|[2006.09920v3](http://arxiv.org/abs/2006.09920v3)|[link](https://github.com/BigRedT/info-ground)|
|**2019-12-01**|**Learning to Relate from Captions and Bounding Boxes**|Sarthak Garg et.al.|[1912.00311v1](http://arxiv.org/abs/1912.00311v1)|null|
|**2019-08-29**|**Aesthetic Image Captioning From Weakly-Labelled Photographs**|Koustav Ghosal et.al.|[1908.11310v1](http://arxiv.org/abs/1908.11310v1)|null|

### Alignment
|Publish Date|Title|Authors|PDF|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-06-09**|**Reinforcing Multimodal Understanding and Generation with Dual Self-rewards**|Jixiang Hong et.al.|[2506.07963v1](http://arxiv.org/abs/2506.07963v1)|null|
|**2025-06-09**|**Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures**|Yukai Zhou et.al.|[2506.07402v1](http://arxiv.org/abs/2506.07402v1)|null|
|**2025-06-08**|**Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning**|Tianyi Bai et.al.|[2506.07227v1](http://arxiv.org/abs/2506.07227v1)|null|
|**2025-06-08**|**Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images**|Liangliang You et.al.|[2506.07184v1](http://arxiv.org/abs/2506.07184v1)|null|
|**2025-06-08**|**Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs**|Wenrui Zhou et.al.|[2506.07180v1](http://arxiv.org/abs/2506.07180v1)|null|
|**2025-06-07**|**Mitigating Object Hallucination via Robust Local Perception Search**|Zixian Gao et.al.|[2506.06729v1](http://arxiv.org/abs/2506.06729v1)|null|
|**2025-06-05**|**Do Large Language Models Judge Error Severity Like Humans?**|Diege Sun et.al.|[2506.05142v2](http://arxiv.org/abs/2506.05142v2)|null|
|**2025-06-05**|**Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics**|HaoTian Lan et.al.|[2506.05087v1](http://arxiv.org/abs/2506.05087v1)|null|
|**2025-06-05**|**Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques**|Jisu An et.al.|[2506.04788v1](http://arxiv.org/abs/2506.04788v1)|null|
|**2025-06-04**|**MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos**|Kejian Zhu et.al.|[2506.04141v1](http://arxiv.org/abs/2506.04141v1)|null|
|**2025-06-04**|**Multimodal Tabular Reasoning with Privileged Structured Information**|Jun-Peng Jiang et.al.|[2506.04088v1](http://arxiv.org/abs/2506.04088v1)|null|
|**2025-06-04**|**A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions**|Chung-Chun Wang et.al.|[2506.04077v1](http://arxiv.org/abs/2506.04077v1)|null|
|**2025-06-04**|**Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization**|Jiulong Wu et.al.|[2506.04039v1](http://arxiv.org/abs/2506.04039v1)|null|
|**2025-06-04**|**Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models**|Junling Wang et.al.|[2506.03735v1](http://arxiv.org/abs/2506.03735v1)|[link](https://github.com/eth-lre/math2visual)|
|**2025-06-02**|**LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment**|Lingyao Li et.al.|[2506.06355v1](http://arxiv.org/abs/2506.06355v1)|null|
|**2025-06-02**|**Generate, Not Recommend: Personalized Multimodal Content Generation**|Jiongnan Liu et.al.|[2506.01704v1](http://arxiv.org/abs/2506.01704v1)|null|
|**2025-06-02**|**FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents**|Bobo Li et.al.|[2506.01520v1](http://arxiv.org/abs/2506.01520v1)|null|
|**2025-06-02**|**Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner**|Chunhui Zhang et.al.|[2506.01301v1](http://arxiv.org/abs/2506.01301v1)|null|
|**2025-06-01**|**Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues**|Youngmin Kim et.al.|[2506.00958v1](http://arxiv.org/abs/2506.00958v1)|null|
|**2025-06-01**|**TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment**|Taesoo Kim et.al.|[2506.06343v1](http://arxiv.org/abs/2506.06343v1)|null|
|**2025-05-31**|**Chain-of-Thought Training for Open E2E Spoken Dialogue Systems**|Siddhant Arora et.al.|[2506.00722v1](http://arxiv.org/abs/2506.00722v1)|null|
|**2025-05-31**|**Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities**|Jiahui Geng et.al.|[2506.00548v1](http://arxiv.org/abs/2506.00548v1)|null|
|**2025-05-31**|**Enhancing Multimodal Continual Instruction Tuning with BranchLoRA**|Duzhen Zhang et.al.|[2506.02041v1](http://arxiv.org/abs/2506.02041v1)|null|
|**2025-05-30**|**CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation**|Emilio Villa-Cueva et.al.|[2505.24456v1](http://arxiv.org/abs/2505.24456v1)|null|
|**2025-05-30**|**KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval**|Fanhang Man et.al.|[2505.24342v1](http://arxiv.org/abs/2505.24342v1)|null|
|**2025-05-29**|**VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos**|Tingyu Song et.al.|[2505.23693v1](http://arxiv.org/abs/2505.23693v1)|[link](https://github.com/sighingsnow/vf-eval)|
|**2025-05-29**|**MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration**|Zhitao He et.al.|[2505.23224v2](http://arxiv.org/abs/2505.23224v2)|[link](https://github.com/zhitao-he/mmboundary)|
|**2025-05-29**|**Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios**|Linjie Mu et.al.|[2505.23118v1](http://arxiv.org/abs/2505.23118v1)|null|
|**2025-05-29**|**SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services**|Hongcheng Guo et.al.|[2505.23065v1](http://arxiv.org/abs/2505.23065v1)|[link](https://github.com/hc-guo/sns-bench-vl)|
|**2025-05-28**|**Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions**|Yijun Shen et.al.|[2505.22627v2](http://arxiv.org/abs/2505.22627v2)|null|
|**2025-05-28**|**Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language**|Guangfu Hao et.al.|[2505.22146v2](http://arxiv.org/abs/2505.22146v2)|null|
|**2025-05-28**|**Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model**|Jintao Zhang et.al.|[2505.22116v2](http://arxiv.org/abs/2505.22116v2)|null|
|**2025-05-28**|**Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset**|Fakhraddin Alwajih et.al.|[2505.21979v1](http://arxiv.org/abs/2505.21979v1)|null|
|**2025-05-28**|**Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack**|Juan Ren et.al.|[2505.21967v1](http://arxiv.org/abs/2505.21967v1)|null|
|**2025-05-28**|**Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation**|Mengdan Zhu et.al.|[2505.21956v2](http://arxiv.org/abs/2505.21956v2)|[link](https://github.com/mengdanzhu/cross-modal-rag)|
|**2025-05-27**|**Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers**|Wei Pang et.al.|[2505.21497v1](http://arxiv.org/abs/2505.21497v1)|[link](https://github.com/paper2poster/paper2poster)|
|**2025-05-27**|**Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration**|Mehrdad Fazli et.al.|[2505.21472v1](http://arxiv.org/abs/2505.21472v1)|null|
|**2025-05-27**|**Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction**|Sam O'Connor Russell et.al.|[2505.21043v1](http://arxiv.org/abs/2505.21043v1)|[link](https://github.com/russelsa/mm-vap)|
|**2025-05-27**|**MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding**|Fuwen Luo et.al.|[2505.20715v1](http://arxiv.org/abs/2505.20715v1)|null|
|**2025-05-26**|**Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting**|Ana Rita Ortigoso et.al.|[2505.20521v1](http://arxiv.org/abs/2505.20521v1)|null|
|**2025-05-26**|**What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models**|Lorenzo Baraldi et.al.|[2505.20405v1](http://arxiv.org/abs/2505.20405v1)|null|
|**2025-05-26**|**VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction**|Zhiwen Fan et.al.|[2505.20279v2](http://arxiv.org/abs/2505.20279v2)|[link](https://github.com/VITA-Group/VLM-3R)|
|**2025-05-26**|**Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion**|Zheqi Lv et.al.|[2505.20053v1](http://arxiv.org/abs/2505.20053v1)|[link](https://github.com/hellozicky/ppad)|
|**2025-05-26**|**ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs**|Pooneh Mousavi et.al.|[2505.19937v1](http://arxiv.org/abs/2505.19937v1)|null|
|**2025-05-26**|**T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search**|Xing Cui et.al.|[2505.19768v1](http://arxiv.org/abs/2505.19768v1)|null|
|**2025-05-26**|**FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models**|Jintao Tong et.al.|[2505.19536v2](http://arxiv.org/abs/2505.19536v2)|[link](https://github.com/tungchintao/flowcut)|
|**2025-05-25**|**LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models**|Aida Kostikova et.al.|[2505.19240v2](http://arxiv.org/abs/2505.19240v2)|null|
|**2025-05-25**|**DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding**|Yunhai Hu et.al.|[2505.19201v2](http://arxiv.org/abs/2505.19201v2)|[link](https://github.com/sai-lab-nyu/dream)|
|**2025-05-25**|**SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs**|Firoj Alam et.al.|[2505.19163v1](http://arxiv.org/abs/2505.19163v1)|null|
|**2025-05-25**|**ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning**|Yeyuan Wang et.al.|[2505.19100v1](http://arxiv.org/abs/2505.19100v1)|null|
|**2025-05-25**|**Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation**|Daniel Csizmadia et.al.|[2505.21549v3](http://arxiv.org/abs/2505.21549v3)|null|
|**2025-05-25**|**STRICT: Stress Test of Rendering Images Containing Text**|Tianyu Zhang et.al.|[2505.18985v1](http://arxiv.org/abs/2505.18985v1)|[link](https://github.com/tianyu-z/strict-bench)|
|**2025-05-24**|**Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework**|Binhao Ma et.al.|[2505.18864v1](http://arxiv.org/abs/2505.18864v1)|[link](https://github.com/Magic-Ma-tech/Audio-Jailbreak-Attacks)|
|**2025-05-24**|**Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps**|Sicheng Feng et.al.|[2505.18675v2](http://arxiv.org/abs/2505.18675v2)|null|
|**2025-05-24**|**MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation**|Woohyun Cho et.al.|[2505.18614v2](http://arxiv.org/abs/2505.18614v2)|[link](https://github.com/k1064190/MAVL)|
|**2025-05-24**|**Flex-Judge: Think Once, Judge Anywhere**|Jongwoo Ko et.al.|[2505.18601v1](http://arxiv.org/abs/2505.18601v1)|[link](https://github.com/jongwooko/flex-judge)|
|**2025-05-23**|**T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation**|Zi-Ao Ma et.al.|[2505.17897v1](http://arxiv.org/abs/2505.17897v1)|null|
|**2025-05-23**|**EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications**|Ancheng Xu et.al.|[2505.17654v2](http://arxiv.org/abs/2505.17654v2)|null|
|**2025-05-23**|**HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning**|Chuhao Zhou et.al.|[2505.17645v1](http://arxiv.org/abs/2505.17645v1)|null|
|**2025-05-23**|**Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks**|Sara Ketabi et.al.|[2505.17643v1](http://arxiv.org/abs/2505.17643v1)|null|
|**2025-05-23**|**MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation**|Jihan Yao et.al.|[2505.17613v1](http://arxiv.org/abs/2505.17613v1)|null|
|**2025-05-22**|**Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models**|Jiachen Jiang et.al.|[2505.17316v1](http://arxiv.org/abs/2505.17316v1)|null|
|**2025-05-22**|**LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning**|Zebin You et.al.|[2505.16933v2](http://arxiv.org/abs/2505.16933v2)|null|
|**2025-05-22**|**IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models**|Yiming Gao et.al.|[2505.16774v1](http://arxiv.org/abs/2505.16774v1)|[link](https://github.com/audiollms/audiobench)|
|**2025-05-22**|**$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion**|Jing Bi et.al.|[2505.16425v1](http://arxiv.org/abs/2505.16425v1)|null|
|**2025-05-22**|**Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation**|Zhenglin Hua et.al.|[2505.16146v1](http://arxiv.org/abs/2505.16146v1)|null|
|**2025-05-21**|**Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition**|Dong Won Lee et.al.|[2505.15922v1](http://arxiv.org/abs/2505.15922v1)|null|

## Computer Vision

### OVD
|Publish Date|Title|Authors|PDF|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-03-21**|**An Iterative Feedback Mechanism for Improving Natural Language Class Descriptions in Open-Vocabulary Object Detection**|Louis Y. Kim et.al.|[2503.17285v1](http://arxiv.org/abs/2503.17285v1)|null|
|**2024-10-27**|**Open-Vocabulary Object Detection via Language Hierarchy**|Jiaxing Huang et.al.|[2410.20371v1](http://arxiv.org/abs/2410.20371v1)|null|
|**2024-09-24**|**HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear Composition for Open-Vocabulary Object Detection**|Yuqi Ma et.al.|[2409.16136v1](http://arxiv.org/abs/2409.16136v1)|null|
|**2024-07-03**|**BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs**|Zhantao Yang et.al.|[2407.03314v2](http://arxiv.org/abs/2407.03314v2)|null|
|**2024-04-03**|**ALOHa: A New Measure for Hallucination in Captioning Models**|Suzanne Petryk et.al.|[2404.02904v1](http://arxiv.org/abs/2404.02904v1)|null|
|**2024-03-21**|**Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection**|Tim Salzmann et.al.|[2403.14270v2](http://arxiv.org/abs/2403.14270v2)|null|
|**2024-03-11**|**Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head**|Tiancheng Zhao et.al.|[2403.06892v2](http://arxiv.org/abs/2403.06892v2)|[link](https://github.com/om-ai-lab/OmDet)|
|**2023-08-25**|**How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection**|Yiyang Yao et.al.|[2308.13177v2](http://arxiv.org/abs/2308.13177v2)|[link](https://github.com/om-ai-lab/ovdeval)|

### LMM
|Publish Date|Title|Authors|PDF|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-06-09**|**Play to Generalize: Learning to Reason Through Game Play**|Yunfei Xie et.al.|[2506.08011v1](http://arxiv.org/abs/2506.08011v1)|null|
|**2025-06-09**|**Reinforcing Multimodal Understanding and Generation with Dual Self-rewards**|Jixiang Hong et.al.|[2506.07963v1](http://arxiv.org/abs/2506.07963v1)|null|
|**2025-06-09**|**WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code**|Zhiyu Lin et.al.|[2506.07818v1](http://arxiv.org/abs/2506.07818v1)|null|
|**2025-06-09**|**Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures**|Yukai Zhou et.al.|[2506.07402v1](http://arxiv.org/abs/2506.07402v1)|null|
|**2025-06-08**|**Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning**|Tianyi Bai et.al.|[2506.07227v1](http://arxiv.org/abs/2506.07227v1)|null|
|**2025-06-08**|**SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning**|Mengya Xu et.al.|[2506.07196v1](http://arxiv.org/abs/2506.07196v1)|null|
|**2025-06-08**|**Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images**|Liangliang You et.al.|[2506.07184v1](http://arxiv.org/abs/2506.07184v1)|null|
|**2025-06-08**|**Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs**|Wenrui Zhou et.al.|[2506.07180v1](http://arxiv.org/abs/2506.07180v1)|null|
|**2025-06-08**|**Learning Compact Vision Tokens for Efficient Large Multimodal Models**|Hao Tang et.al.|[2506.07138v1](http://arxiv.org/abs/2506.07138v1)|[link](https://github.com/visresearch/LLaVA-STF)|
|**2025-06-08**|**Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning**|LASA Team et.al.|[2506.07044v2](http://arxiv.org/abs/2506.07044v2)|null|
|**2025-06-08**|**A Culturally-diverse Multilingual Multimodal Video Benchmark & Model**|Bhuiyan Sanjid Shafique et.al.|[2506.07032v1](http://arxiv.org/abs/2506.07032v1)|null|
|**2025-06-08**|**Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors**|Senqi Yang et.al.|[2506.06987v1](http://arxiv.org/abs/2506.06987v1)|null|
|**2025-06-07**|**Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering**|Akash Gupta et.al.|[2506.06905v2](http://arxiv.org/abs/2506.06905v2)|null|
|**2025-06-07**|**Mitigating Object Hallucination via Robust Local Perception Search**|Zixian Gao et.al.|[2506.06729v1](http://arxiv.org/abs/2506.06729v1)|null|
|**2025-06-06**|**Can Theoretical Physics Research Benefit from Language Agents?**|Sirui Lu et.al.|[2506.06214v1](http://arxiv.org/abs/2506.06214v1)|null|
|**2025-06-06**|**PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts**|Hengzhi Li et.al.|[2506.06211v1](http://arxiv.org/abs/2506.06211v1)|null|
|**2025-06-06**|**SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities**|Guoyang Xia et.al.|[2506.06406v1](http://arxiv.org/abs/2506.06406v1)|null|
|**2025-06-06**|**MATP-BENCH: Can MLLM Be a Good Automated Theorem Prover for Multimodal Problems?**|Zhitao He et.al.|[2506.06034v1](http://arxiv.org/abs/2506.06034v1)|null|
|**2025-06-06**|**BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over Bio-Molecular Interactions**|Saptarshi Sengupta et.al.|[2506.05766v1](http://arxiv.org/abs/2506.05766v1)|null|
|**2025-06-05**|**MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning**|Zikui Cai et.al.|[2506.05523v1](http://arxiv.org/abs/2506.05523v1)|null|
|**2025-06-05**|**Unleashing Hour-Scale Video Training for Long Video-Language Understanding**|Jingyang Lin et.al.|[2506.05332v1](http://arxiv.org/abs/2506.05332v1)|null|
|**2025-06-05**|**MLLM-CL: Continual Learning for Multimodal Large Language Models**|Hongbo Zhao et.al.|[2506.05453v1](http://arxiv.org/abs/2506.05453v1)|null|
|**2025-06-05**|**Do Large Language Models Judge Error Severity Like Humans?**|Diege Sun et.al.|[2506.05142v2](http://arxiv.org/abs/2506.05142v2)|null|
|**2025-06-05**|**The NTNU System at the S&I Challenge 2025 SLA Open Track**|Hong-Yun Lin et.al.|[2506.05121v1](http://arxiv.org/abs/2506.05121v1)|null|
|**2025-06-05**|**Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics**|HaoTian Lan et.al.|[2506.05087v1](http://arxiv.org/abs/2506.05087v1)|null|
|**2025-06-05**|**Parking, Perception, and Retail: Street-Level Determinants of Community Vitality in Harbin**|HaoTian Lan et.al.|[2506.05080v1](http://arxiv.org/abs/2506.05080v1)|null|
|**2025-06-05**|**LLMs Can Compensate for Deficiencies in Visual Representations**|Sho Takishita et.al.|[2506.05439v1](http://arxiv.org/abs/2506.05439v1)|null|
|**2025-06-05**|**Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques**|Jisu An et.al.|[2506.04788v1](http://arxiv.org/abs/2506.04788v1)|null|
|**2025-06-05**|**MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning Benchmark**|Dingdong Wang et.al.|[2506.04779v1](http://arxiv.org/abs/2506.04779v1)|[link](https://github.com/dingdongwang/mmsu_bench)|
|**2025-06-05**|**MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models**|Gio Paik et.al.|[2506.04688v1](http://arxiv.org/abs/2506.04688v1)|null|
|**2025-06-05**|**MuSciClaims: Multimodal Scientific Claim Verification**|Yash Kumar Lal et.al.|[2506.04585v1](http://arxiv.org/abs/2506.04585v1)|null|
|**2025-06-05**|**From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems**|Jiayi Chen et.al.|[2506.04565v1](http://arxiv.org/abs/2506.04565v1)|null|
|**2025-06-04**|**ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding**|Ankit Pal et.al.|[2506.04353v1](http://arxiv.org/abs/2506.04353v1)|null|
|**2025-06-04**|**Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning**|Shuang Chen et.al.|[2506.04207v1](http://arxiv.org/abs/2506.04207v1)|null|
|**2025-06-04**|**MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos**|Kejian Zhu et.al.|[2506.04141v1](http://arxiv.org/abs/2506.04141v1)|null|
|**2025-06-04**|**Multimodal Tabular Reasoning with Privileged Structured Information**|Jun-Peng Jiang et.al.|[2506.04088v1](http://arxiv.org/abs/2506.04088v1)|null|
|**2025-06-04**|**A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions**|Chung-Chun Wang et.al.|[2506.04077v1](http://arxiv.org/abs/2506.04077v1)|null|
|**2025-06-04**|**Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization**|Jiulong Wu et.al.|[2506.04039v1](http://arxiv.org/abs/2506.04039v1)|null|
|**2025-06-04**|**Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era**|Dan Oneata et.al.|[2506.03994v1](http://arxiv.org/abs/2506.03994v1)|null|
|**2025-06-04**|**HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models**|Zhaolu Kang et.al.|[2506.03922v1](http://arxiv.org/abs/2506.03922v1)|null|
|**2025-06-03**|**A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation**|Zihui Ma et.al.|[2506.03360v1](http://arxiv.org/abs/2506.03360v1)|[link](https://github.com/missa7481/emnlp25_earthquake)|
|**2025-06-03**|**UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation**|Bin Lin et.al.|[2506.03147v3](http://arxiv.org/abs/2506.03147v3)|null|
|**2025-06-03**|**CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG**|Yang Tian et.al.|[2506.02544v2](http://arxiv.org/abs/2506.02544v2)|null|
|**2025-06-03**|**Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text**|Junzhe Zhang et.al.|[2506.02494v1](http://arxiv.org/abs/2506.02494v1)|null|
|**2025-06-03**|**Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework**|Zhaorui Yang et.al.|[2506.02454v1](http://arxiv.org/abs/2506.02454v1)|null|
|**2025-06-02**|**LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment**|Lingyao Li et.al.|[2506.06355v1](http://arxiv.org/abs/2506.06355v1)|null|
|**2025-06-02**|**SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning**|Zhongwei Wan et.al.|[2506.01713v1](http://arxiv.org/abs/2506.01713v1)|null|
|**2025-06-02**|**Generate, Not Recommend: Personalized Multimodal Content Generation**|Jiongnan Liu et.al.|[2506.01704v1](http://arxiv.org/abs/2506.01704v1)|null|
|**2025-06-02**|**Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents**|Shuting Wang et.al.|[2506.01689v1](http://arxiv.org/abs/2506.01689v1)|null|
|**2025-06-02**|**FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents**|Bobo Li et.al.|[2506.01520v1](http://arxiv.org/abs/2506.01520v1)|null|
|**2025-06-02**|**MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions**|Tung-Lam Ngo et.al.|[2506.01478v1](http://arxiv.org/abs/2506.01478v1)|null|
|**2025-06-02**|**Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner**|Chunhui Zhang et.al.|[2506.01301v1](http://arxiv.org/abs/2506.01301v1)|null|
|**2025-06-01**|**Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues**|Youngmin Kim et.al.|[2506.00958v1](http://arxiv.org/abs/2506.00958v1)|null|
|**2025-06-01**|**Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection**|Zhu Li et.al.|[2506.00955v1](http://arxiv.org/abs/2506.00955v1)|null|
|**2025-06-01**|**anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding**|Haitao Li et.al.|[2506.00942v1](http://arxiv.org/abs/2506.00942v1)|null|
|**2025-06-01**|**TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment**|Taesoo Kim et.al.|[2506.06343v1](http://arxiv.org/abs/2506.06343v1)|null|
|**2025-06-01**|**Affordance Benchmark for MLLMs**|Junying Wang et.al.|[2506.00893v1](http://arxiv.org/abs/2506.00893v1)|null|
|**2025-06-01**|**Improve MLLM Benchmark Efficiency through Interview**|Farong Wen et.al.|[2506.00883v1](http://arxiv.org/abs/2506.00883v1)|null|
|**2025-06-01**|**HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs**|Yongkang Xiao et.al.|[2506.00826v1](http://arxiv.org/abs/2506.00826v1)|null|
|**2025-06-01**|**Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering**|Songtao Jiang et.al.|[2506.00806v1](http://arxiv.org/abs/2506.00806v1)|null|
|**2025-05-31**|**Chain-of-Thought Training for Open E2E Spoken Dialogue Systems**|Siddhant Arora et.al.|[2506.00722v1](http://arxiv.org/abs/2506.00722v1)|null|
|**2025-05-31**|**MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning**|Peng Xia et.al.|[2506.00555v1](http://arxiv.org/abs/2506.00555v1)|null|
|**2025-05-31**|**Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities**|Jiahui Geng et.al.|[2506.00548v1](http://arxiv.org/abs/2506.00548v1)|null|
|**2025-05-31**|**Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection**|Shuguo Hu et.al.|[2506.00488v1](http://arxiv.org/abs/2506.00488v1)|null|
|**2025-05-31**|**Enhancing Multimodal Continual Instruction Tuning with BranchLoRA**|Duzhen Zhang et.al.|[2506.02041v1](http://arxiv.org/abs/2506.02041v1)|null|
|**2025-05-30**|**ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment**|Ehsan Karimi et.al.|[2506.00238v1](http://arxiv.org/abs/2506.00238v1)|null|
