# arxiv-daily
 Automated deployment @ 2025-06-23 10:10:57
> Add your topics and keywords in `database/topic.yml` 
> You can also view historical data through the `database/storage` 

## Computer Vision

### OVD
|Publish Date|Title|Authors|PDF|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-03-21**|**An Iterative Feedback Mechanism for Improving Natural Language Class Descriptions in Open-Vocabulary Object Detection**|Louis Y. Kim et.al.|[2503.17285v1](http://arxiv.org/abs/2503.17285v1)|null|
|**2024-10-27**|**Open-Vocabulary Object Detection via Language Hierarchy**|Jiaxing Huang et.al.|[2410.20371v1](http://arxiv.org/abs/2410.20371v1)|null|
|**2024-09-24**|**HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear Composition for Open-Vocabulary Object Detection**|Yuqi Ma et.al.|[2409.16136v1](http://arxiv.org/abs/2409.16136v1)|null|
|**2024-07-03**|**BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs**|Zhantao Yang et.al.|[2407.03314v2](http://arxiv.org/abs/2407.03314v2)|null|
|**2024-04-03**|**ALOHa: A New Measure for Hallucination in Captioning Models**|Suzanne Petryk et.al.|[2404.02904v1](http://arxiv.org/abs/2404.02904v1)|null|
|**2024-03-21**|**Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection**|Tim Salzmann et.al.|[2403.14270v2](http://arxiv.org/abs/2403.14270v2)|null|
|**2024-03-11**|**Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head**|Tiancheng Zhao et.al.|[2403.06892v2](http://arxiv.org/abs/2403.06892v2)|[link](https://github.com/om-ai-lab/OmDet)|

### LMM
|Publish Date|Title|Authors|PDF|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-06-20**|**MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation**|Shoubin Yu et.al.|[2506.17113v1](http://arxiv.org/abs/2506.17113v1)|null|
|**2025-06-20**|**MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models**|Xiaolong Wang et.al.|[2506.17046v1](http://arxiv.org/abs/2506.17046v1)|null|
|**2025-06-20**|**Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs**|Haoran Sun et.al.|[2506.16962v1](http://arxiv.org/abs/2506.16962v1)|null|
|**2025-06-20**|**Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models**|Lei Jiang et.al.|[2506.16760v1](http://arxiv.org/abs/2506.16760v1)|null|
|**2025-06-19**|**A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications**|Hanshu Rao et.al.|[2506.16594v1](http://arxiv.org/abs/2506.16594v1)|null|
|**2025-06-19**|**GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning**|Yi Chen et.al.|[2506.16141v1](http://arxiv.org/abs/2506.16141v1)|null|
|**2025-06-18**|**WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts**|Negar Foroutan et.al.|[2506.15594v1](http://arxiv.org/abs/2506.15594v1)|[link](https://github.com/negar-foroutan/wikimixqa)|
|**2025-06-18**|**Understanding GUI Agent Localization Biases through Logit Sharpness**|Xingjian Tao et.al.|[2506.15425v1](http://arxiv.org/abs/2506.15425v1)|null|
|**2025-06-18**|**InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding**|Minsoo Kim et.al.|[2506.15745v1](http://arxiv.org/abs/2506.15745v1)|null|
|**2025-06-17**|**ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM**|Yujun Wang et.al.|[2506.14766v1](http://arxiv.org/abs/2506.14766v1)|null|
|**2025-06-17**|**M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models**|Can Zheng et.al.|[2506.14532v1](http://arxiv.org/abs/2506.14532v1)|null|
|**2025-06-17**|**LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops**|Jiyuan Fu et.al.|[2506.14493v1](http://arxiv.org/abs/2506.14493v1)|null|
|**2025-06-17**|**RadFabric: Agentic AI System with Reasoning Capability for Radiology**|Wenting Chen et.al.|[2506.14142v1](http://arxiv.org/abs/2506.14142v1)|null|
|**2025-06-16**|**MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation**|Xueqing Peng et.al.|[2506.14028v2](http://arxiv.org/abs/2506.14028v2)|null|
|**2025-06-16**|**ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection**|Shang-Chi Tsai et.al.|[2506.13956v1](http://arxiv.org/abs/2506.13956v1)|null|
|**2025-06-16**|**VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training**|Jipeng Zhang et.al.|[2506.13888v1](http://arxiv.org/abs/2506.13888v1)|null|
|**2025-06-16**|**Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model**|Shaolei Zhang et.al.|[2506.13642v1](http://arxiv.org/abs/2506.13642v1)|[link](https://github.com/ictnlp/stream-omni)|
|**2025-06-16**|**RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating LLM-Based Table Analysis**|Pengzuo Wu et.al.|[2506.13405v1](http://arxiv.org/abs/2506.13405v1)|null|
|**2025-06-16**|**ZINA: Multimodal Fine-grained Hallucination Detection and Editing**|Yuiga Wada et.al.|[2506.13130v1](http://arxiv.org/abs/2506.13130v1)|null|
|**2025-06-16**|**FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design**|Kai Lan et.al.|[2506.13066v1](http://arxiv.org/abs/2506.13066v1)|null|
|**2025-06-16**|**CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large Language Model**|Jiangtong Li et.al.|[2506.13055v1](http://arxiv.org/abs/2506.13055v1)|null|
|**2025-06-16**|**Stress-Testing Multimodal Foundation Models for Crystallographic Reasoning**|Can Polat et.al.|[2506.13051v1](http://arxiv.org/abs/2506.13051v1)|[link](https://github.com/kurbanintelligencelab/stresstestingmmfmincr)|
|**2025-06-14**|**RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking**|Shuo Yang et.al.|[2506.12538v1](http://arxiv.org/abs/2506.12538v1)|null|
|**2025-06-14**|**FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation**|Zhuocheng Zhang et.al.|[2506.12494v1](http://arxiv.org/abs/2506.12494v1)|[link](https://github.com/ictnlp/flexrag)|
|**2025-06-14**|**Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics**|Asifullah khan et.al.|[2506.12365v1](http://arxiv.org/abs/2506.12365v1)|null|
|**2025-06-14**|**Perspective on Utilizing Foundation Models for Laboratory Automation in Materials Research**|Kan Hatakeyama-Sato et.al.|[2506.12312v1](http://arxiv.org/abs/2506.12312v1)|null|
|**2025-06-13**|**VGR: Visual Grounded Reasoning**|Jiacong Wang et.al.|[2506.11991v2](http://arxiv.org/abs/2506.11991v2)|null|
|**2025-06-13**|**Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?**|Simeon Junker et.al.|[2506.11807v1](http://arxiv.org/abs/2506.11807v1)|null|
|**2025-06-13**|**DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs**|Bo-Cheng Chiu et.al.|[2506.11558v1](http://arxiv.org/abs/2506.11558v1)|null|
|**2025-06-13**|**Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs**|Xiao Xu et.al.|[2506.11515v1](http://arxiv.org/abs/2506.11515v1)|null|
|**2025-06-13**|**Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables**|Yitong Zhou et.al.|[2506.11375v1](http://arxiv.org/abs/2506.11375v1)|null|
|**2025-06-12**|**Build the web for agents, not agents for the web**|Xing Han LÃ¹ et.al.|[2506.10953v1](http://arxiv.org/abs/2506.10953v1)|null|
|**2025-06-12**|**Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?**|Fei Lin et.al.|[2506.10912v2](http://arxiv.org/abs/2506.10912v2)|null|
|**2025-06-12**|**VideoDeepResearch: Long Video Understanding With Agentic Tool Using**|Huaying Yuan et.al.|[2506.10821v2](http://arxiv.org/abs/2506.10821v2)|[link](https://github.com/yhy-2000/videodeepresearch)|
|**2025-06-12**|**Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning**|Yuhao Zhou et.al.|[2506.10521v2](http://arxiv.org/abs/2506.10521v2)|null|
|**2025-06-12**|**Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?**|Yingjin Song et.al.|[2506.10415v1](http://arxiv.org/abs/2506.10415v1)|null|
|**2025-06-11**|**Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval**|Shubhashis Roy Dipta et.al.|[2506.10202v1](http://arxiv.org/abs/2506.10202v1)|null|
|**2025-06-11**|**ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering**|Caijun Jia et.al.|[2506.10116v1](http://arxiv.org/abs/2506.10116v1)|null|
|**2025-06-11**|**Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs**|Beomsik Cho et.al.|[2506.09522v1](http://arxiv.org/abs/2506.09522v1)|[link](https://github.com/bscho333/ReVisiT)|
|**2025-06-11**|**Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings**|Md Messal Monem Miah et.al.|[2506.09424v1](http://arxiv.org/abs/2506.09424v1)|null|
|**2025-06-10**|**PHRASED: Phrase Dictionary Biasing for Speech Translation**|Peidong Wang et.al.|[2506.09175v1](http://arxiv.org/abs/2506.09175v1)|null|
|**2025-06-10**|**Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better**|Dianyi Wang et.al.|[2506.09040v1](http://arxiv.org/abs/2506.09040v1)|[link](https://github.com/alenjandrowang/asvr)|
|**2025-06-10**|**Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System**|Yuan Guo et.al.|[2506.08972v1](http://arxiv.org/abs/2506.08972v1)|null|
|**2025-06-10**|**ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts**|Ruiran Su et.al.|[2506.08700v2](http://arxiv.org/abs/2506.08700v2)|[link](https://github.com/albasu120491/climateviz)|
|**2025-06-10**|**Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning**|Fengjun Pan et.al.|[2506.08477v1](http://arxiv.org/abs/2506.08477v1)|null|
|**2025-06-10**|**mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks**|Luel Hagos Beyene et.al.|[2506.08400v1](http://arxiv.org/abs/2506.08400v1)|null|
|**2025-06-10**|**Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency**|Chenlong Wang et.al.|[2506.08343v2](http://arxiv.org/abs/2506.08343v2)|null|
|**2025-06-09**|**Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain**|Subba Reddy Oota et.al.|[2506.08277v1](http://arxiv.org/abs/2506.08277v1)|[link](https://github.com/subbareddy248/mllm_videos)|
|**2025-06-09**|**Open World Scene Graph Generation using Vision Language Models**|Amartya Dutta et.al.|[2506.08189v1](http://arxiv.org/abs/2506.08189v1)|[link](https://github.com/shtuplus/pix2grp_cvpr2024)|
|**2025-06-09**|**EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments**|Zefang Liu et.al.|[2506.08136v1](http://arxiv.org/abs/2506.08136v1)|null|
|**2025-06-09**|**Play to Generalize: Learning to Reason Through Game Play**|Yunfei Xie et.al.|[2506.08011v2](http://arxiv.org/abs/2506.08011v2)|[link](https://github.com/yunfeixie233/vigal)|
|**2025-06-09**|**Reinforcing Multimodal Understanding and Generation with Dual Self-rewards**|Jixiang Hong et.al.|[2506.07963v2](http://arxiv.org/abs/2506.07963v2)|null|
|**2025-06-09**|**WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code**|Zhiyu Lin et.al.|[2506.07818v1](http://arxiv.org/abs/2506.07818v1)|[link](https://github.com/mail-tele-ai/webuibench)|
|**2025-06-09**|**Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures**|Yukai Zhou et.al.|[2506.07402v1](http://arxiv.org/abs/2506.07402v1)|null|
|**2025-06-09**|**KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations**|Junyu Liu et.al.|[2506.11114v1](http://arxiv.org/abs/2506.11114v1)|null|
|**2025-06-08**|**Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning**|Tianyi Bai et.al.|[2506.07227v1](http://arxiv.org/abs/2506.07227v1)|null|
|**2025-06-08**|**SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning**|Mengya Xu et.al.|[2506.07196v2](http://arxiv.org/abs/2506.07196v2)|null|
|**2025-06-08**|**Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images**|Liangliang You et.al.|[2506.07184v1](http://arxiv.org/abs/2506.07184v1)|null|
|**2025-06-08**|**Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs**|Wenrui Zhou et.al.|[2506.07180v1](http://arxiv.org/abs/2506.07180v1)|null|
|**2025-06-08**|**Learning Compact Vision Tokens for Efficient Large Multimodal Models**|Hao Tang et.al.|[2506.07138v1](http://arxiv.org/abs/2506.07138v1)|[link](https://github.com/visresearch/LLaVA-STF)|
|**2025-06-08**|**Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning**|LASA Team et.al.|[2506.07044v4](http://arxiv.org/abs/2506.07044v4)|null|
|**2025-06-08**|**A Culturally-diverse Multilingual Multimodal Video Benchmark & Model**|Bhuiyan Sanjid Shafique et.al.|[2506.07032v1](http://arxiv.org/abs/2506.07032v1)|null|

## Mutimodal

### Alignment
|Publish Date|Title|Authors|PDF|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-06-20**|**MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models**|Xiaolong Wang et.al.|[2506.17046v1](http://arxiv.org/abs/2506.17046v1)|null|
|**2025-06-20**|**Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models**|Lei Jiang et.al.|[2506.16760v1](http://arxiv.org/abs/2506.16760v1)|null|
|**2025-06-18**|**Understanding GUI Agent Localization Biases through Logit Sharpness**|Xingjian Tao et.al.|[2506.15425v1](http://arxiv.org/abs/2506.15425v1)|null|
|**2025-06-17**|**M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models**|Can Zheng et.al.|[2506.14532v1](http://arxiv.org/abs/2506.14532v1)|null|
|**2025-06-17**|**RadFabric: Agentic AI System with Reasoning Capability for Radiology**|Wenting Chen et.al.|[2506.14142v1](http://arxiv.org/abs/2506.14142v1)|null|
|**2025-06-16**|**VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training**|Jipeng Zhang et.al.|[2506.13888v1](http://arxiv.org/abs/2506.13888v1)|null|
|**2025-06-16**|**Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model**|Shaolei Zhang et.al.|[2506.13642v1](http://arxiv.org/abs/2506.13642v1)|[link](https://github.com/ictnlp/stream-omni)|
|**2025-06-16**|**FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design**|Kai Lan et.al.|[2506.13066v1](http://arxiv.org/abs/2506.13066v1)|null|
|**2025-06-14**|**Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics**|Asifullah khan et.al.|[2506.12365v1](http://arxiv.org/abs/2506.12365v1)|null|
|**2025-06-13**|**DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs**|Bo-Cheng Chiu et.al.|[2506.11558v1](http://arxiv.org/abs/2506.11558v1)|null|
|**2025-06-13**|**Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs**|Xiao Xu et.al.|[2506.11515v1](http://arxiv.org/abs/2506.11515v1)|null|
|**2025-06-12**|**Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts**|Guowei Zhong et.al.|[2506.10452v1](http://arxiv.org/abs/2506.10452v1)|[link](https://github.com/gw-zhong/cider)|
|**2025-06-10**|**Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better**|Dianyi Wang et.al.|[2506.09040v1](http://arxiv.org/abs/2506.09040v1)|[link](https://github.com/alenjandrowang/asvr)|
|**2025-06-10**|**SensorLM: Learning the Language of Wearable Sensors**|Yuwei Zhang et.al.|[2506.09108v1](http://arxiv.org/abs/2506.09108v1)|null|
|**2025-06-09**|**Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain**|Subba Reddy Oota et.al.|[2506.08277v1](http://arxiv.org/abs/2506.08277v1)|[link](https://github.com/subbareddy248/mllm_videos)|
|**2025-06-09**|**Open World Scene Graph Generation using Vision Language Models**|Amartya Dutta et.al.|[2506.08189v1](http://arxiv.org/abs/2506.08189v1)|[link](https://github.com/shtuplus/pix2grp_cvpr2024)|
|**2025-06-09**|**Reinforcing Multimodal Understanding and Generation with Dual Self-rewards**|Jixiang Hong et.al.|[2506.07963v2](http://arxiv.org/abs/2506.07963v2)|null|
|**2025-06-09**|**Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures**|Yukai Zhou et.al.|[2506.07402v1](http://arxiv.org/abs/2506.07402v1)|null|
|**2025-06-08**|**Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning**|Tianyi Bai et.al.|[2506.07227v1](http://arxiv.org/abs/2506.07227v1)|null|
|**2025-06-08**|**Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images**|Liangliang You et.al.|[2506.07184v1](http://arxiv.org/abs/2506.07184v1)|null|
|**2025-06-08**|**Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs**|Wenrui Zhou et.al.|[2506.07180v1](http://arxiv.org/abs/2506.07180v1)|null|
|**2025-06-07**|**Mitigating Object Hallucination via Robust Local Perception Search**|Zixian Gao et.al.|[2506.06729v1](http://arxiv.org/abs/2506.06729v1)|null|
|**2025-06-05**|**Do Large Language Models Judge Error Severity Like Humans?**|Diege Sun et.al.|[2506.05142v2](http://arxiv.org/abs/2506.05142v2)|null|
|**2025-06-05**|**Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics**|HaoTian Lan et.al.|[2506.05087v1](http://arxiv.org/abs/2506.05087v1)|null|
|**2025-06-05**|**Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques**|Jisu An et.al.|[2506.04788v1](http://arxiv.org/abs/2506.04788v1)|null|
|**2025-06-04**|**MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos**|Kejian Zhu et.al.|[2506.04141v1](http://arxiv.org/abs/2506.04141v1)|null|
|**2025-06-04**|**Multimodal Tabular Reasoning with Privileged Structured Information**|Jun-Peng Jiang et.al.|[2506.04088v1](http://arxiv.org/abs/2506.04088v1)|null|
|**2025-06-04**|**A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions**|Chung-Chun Wang et.al.|[2506.04077v1](http://arxiv.org/abs/2506.04077v1)|null|
|**2025-06-04**|**Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization**|Jiulong Wu et.al.|[2506.04039v1](http://arxiv.org/abs/2506.04039v1)|null|
|**2025-06-04**|**Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models**|Junling Wang et.al.|[2506.03735v1](http://arxiv.org/abs/2506.03735v1)|[link](https://github.com/eth-lre/math2visual)|
|**2025-06-03**|**CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention**|Zekai Ye et.al.|[2506.11073v1](http://arxiv.org/abs/2506.11073v1)|null|
|**2025-06-02**|**LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment**|Lingyao Li et.al.|[2506.06355v1](http://arxiv.org/abs/2506.06355v1)|null|
|**2025-06-02**|**Generate, Not Recommend: Personalized Multimodal Content Generation**|Jiongnan Liu et.al.|[2506.01704v1](http://arxiv.org/abs/2506.01704v1)|null|
|**2025-06-02**|**FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents**|Bobo Li et.al.|[2506.01520v1](http://arxiv.org/abs/2506.01520v1)|null|
|**2025-06-02**|**Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner**|Chunhui Zhang et.al.|[2506.01301v1](http://arxiv.org/abs/2506.01301v1)|null|
|**2025-06-01**|**Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues**|Youngmin Kim et.al.|[2506.00958v1](http://arxiv.org/abs/2506.00958v1)|null|
|**2025-06-01**|**TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment**|Taesoo Kim et.al.|[2506.06343v1](http://arxiv.org/abs/2506.06343v1)|null|
|**2025-05-31**|**Chain-of-Thought Training for Open E2E Spoken Dialogue Systems**|Siddhant Arora et.al.|[2506.00722v1](http://arxiv.org/abs/2506.00722v1)|null|
|**2025-05-31**|**Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities**|Jiahui Geng et.al.|[2506.00548v1](http://arxiv.org/abs/2506.00548v1)|[link](https://github.com/UKPLab/acl2025-con-instruction)|
|**2025-05-31**|**Enhancing Multimodal Continual Instruction Tuning with BranchLoRA**|Duzhen Zhang et.al.|[2506.02041v1](http://arxiv.org/abs/2506.02041v1)|null|
|**2025-05-30**|**CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation**|Emilio Villa-Cueva et.al.|[2505.24456v1](http://arxiv.org/abs/2505.24456v1)|null|
|**2025-05-30**|**KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval**|Fanhang Man et.al.|[2505.24342v1](http://arxiv.org/abs/2505.24342v1)|null|
|**2025-05-29**|**VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos**|Tingyu Song et.al.|[2505.23693v1](http://arxiv.org/abs/2505.23693v1)|[link](https://github.com/sighingsnow/vf-eval)|
|**2025-05-29**|**MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration**|Zhitao He et.al.|[2505.23224v2](http://arxiv.org/abs/2505.23224v2)|[link](https://github.com/zhitao-he/mmboundary)|
|**2025-05-29**|**Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios**|Linjie Mu et.al.|[2505.23118v1](http://arxiv.org/abs/2505.23118v1)|null|
|**2025-05-29**|**SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services**|Hongcheng Guo et.al.|[2505.23065v1](http://arxiv.org/abs/2505.23065v1)|[link](https://github.com/hc-guo/sns-bench-vl)|
|**2025-05-28**|**Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions**|Yijun Shen et.al.|[2505.22627v2](http://arxiv.org/abs/2505.22627v2)|null|
|**2025-05-28**|**Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language**|Guangfu Hao et.al.|[2505.22146v2](http://arxiv.org/abs/2505.22146v2)|null|
|**2025-05-28**|**Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model**|Jintao Zhang et.al.|[2505.22116v2](http://arxiv.org/abs/2505.22116v2)|null|
|**2025-05-28**|**Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset**|Fakhraddin Alwajih et.al.|[2505.21979v1](http://arxiv.org/abs/2505.21979v1)|null|
|**2025-05-28**|**Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack**|Juan Ren et.al.|[2505.21967v1](http://arxiv.org/abs/2505.21967v1)|null|
|**2025-05-28**|**Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation**|Mengdan Zhu et.al.|[2505.21956v2](http://arxiv.org/abs/2505.21956v2)|[link](https://github.com/mengdanzhu/cross-modal-rag)|
|**2025-05-27**|**Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers**|Wei Pang et.al.|[2505.21497v1](http://arxiv.org/abs/2505.21497v1)|[link](https://github.com/paper2poster/paper2poster)|
|**2025-05-27**|**Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration**|Mehrdad Fazli et.al.|[2505.21472v1](http://arxiv.org/abs/2505.21472v1)|null|
|**2025-05-27**|**Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction**|Sam O'Connor Russell et.al.|[2505.21043v1](http://arxiv.org/abs/2505.21043v1)|[link](https://github.com/russelsa/mm-vap)|
|**2025-05-27**|**MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding**|Fuwen Luo et.al.|[2505.20715v1](http://arxiv.org/abs/2505.20715v1)|[link](https://github.com/thunlp-mt/museg)|
|**2025-05-26**|**Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting**|Ana Rita Ortigoso et.al.|[2505.20521v1](http://arxiv.org/abs/2505.20521v1)|null|
|**2025-05-26**|**What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models**|Lorenzo Baraldi et.al.|[2505.20405v1](http://arxiv.org/abs/2505.20405v1)|null|
|**2025-05-26**|**VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction**|Zhiwen Fan et.al.|[2505.20279v2](http://arxiv.org/abs/2505.20279v2)|[link](https://github.com/VITA-Group/VLM-3R)|
